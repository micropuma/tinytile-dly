// -----// IR Dump Before TutorialApplyTilingSpec (tutorial-apply-tiling-spec) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> ()>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %broadcasted = linalg.broadcast ins(%arg2 : tensor<128xf32>) outs(%0 : tensor<5x80x100x128xf32>) dimensions = [0, 1, 2] 
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%arg1, %arg0 : tensor<128x3x3x128xf32>, tensor<5x82x102x128xf32>) outs(%broadcasted : tensor<5x80x100x128xf32>) attrs =  {lowering_config = {parallel = [1, 1, 5, 64], reduction = [0, 0, 0, 0, 1, 1, 1]}} {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %3 = arith.mulf %in, %in_0 fastmath<fast> : f32
      %4 = arith.addf %out, %3 fastmath<fast> : f32
      linalg.yield %4 : f32
    } -> tensor<5x80x100x128xf32>
    %cst = arith.constant 0.000000e+00 : f32
    %2 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %1 : f32, tensor<5x80x100x128xf32>) outs(%arg3 : tensor<5x80x100x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %3 = arith.maxnumf %in, %in_0 fastmath<fast> : f32
      linalg.yield %3 : f32
    } -> tensor<5x80x100x128xf32>
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before InterpreterPass (transform-interpreter) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> ()>
#map4 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %broadcasted = linalg.broadcast ins(%arg2 : tensor<128xf32>) outs(%0 : tensor<5x80x100x128xf32>) dimensions = [0, 1, 2] 
    %1 = linalg.generic {indexing_maps = [#map, #map1, #map2], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%arg1, %arg0 : tensor<128x3x3x128xf32>, tensor<5x82x102x128xf32>) outs(%broadcasted : tensor<5x80x100x128xf32>) attrs =  {lowering_config = {parallel = [1, 1, 5, 64], reduction = [0, 0, 0, 0, 1, 1, 1]}} {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %3 = arith.mulf %in, %in_0 fastmath<fast> : f32
      %4 = arith.addf %out, %3 fastmath<fast> : f32
      linalg.yield %4 : f32
    } -> tensor<5x80x100x128xf32>
    %cst = arith.constant 0.000000e+00 : f32
    %2 = linalg.generic {indexing_maps = [#map3, #map4, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %1 : f32, tensor<5x80x100x128xf32>) outs(%arg3 : tensor<5x80x100x128xf32>) {
    ^bb0(%in: f32, %in_0: f32, %out: f32):
      %3 = arith.maxnumf %in, %in_0 fastmath<fast> : f32
      linalg.yield %3 : f32
    } -> tensor<5x80x100x128xf32>
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After InterpreterPass (transform-interpreter) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map5 = affine_map<(d0, d1, d2, d3) -> ()>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %broadcasted = linalg.broadcast ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) dimensions = [0, 1, 2] 
        %10 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %cst_5 = arith.constant 0.000000e+00 : f32
        %11 = linalg.fill ins(%cst_5 : f32) outs(%10 : tensor<1x1x5x64x1x1x1xf32>) -> tensor<1x1x5x64x1x1x1xf32>
        %c0 = arith.constant 0 : index
        %c0_6 = arith.constant 0 : index
        %c0_7 = arith.constant 0 : index
        %c3 = arith.constant 3 : index
        %c3_8 = arith.constant 3 : index
        %c128 = arith.constant 128 : index
        %c1 = arith.constant 1 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %12 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %11) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0_6 to %c3_8 step %c1_9 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0_7 to %c128 step %c1_10 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_12 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_14 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<1x1x5x64x1x1x1xf32>
              %16 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_12, %extracted_slice_13 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%extracted_slice_14 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_15 fastmath<fast> : f32
                %18 = arith.addf %out, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              %inserted_slice = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %reduced = linalg.reduce ins(%12 : tensor<1x1x5x64x1x1x1xf32>) outs(%broadcasted : tensor<1x1x5x64xf32>) dimensions = [4, 5, 6] 
          (%in: f32, %init: f32) {
            %14 = arith.addf %in, %init fastmath<fast> : f32
            linalg.yield %14 : f32
          }
        %extracted_slice_11 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %13 = linalg.generic {indexing_maps = [#map5, #map6, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %reduced : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_11 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_12: f32, %out: f32):
          %14 = arith.maxnumf %in, %in_12 fastmath<fast> : f32
          linalg.yield %14 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After TutorialApplyTilingSpec (tutorial-apply-tiling-spec) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map5 = affine_map<(d0, d1, d2, d3) -> ()>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %broadcasted = linalg.broadcast ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) dimensions = [0, 1, 2] 
        %10 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %cst_5 = arith.constant 0.000000e+00 : f32
        %11 = linalg.fill ins(%cst_5 : f32) outs(%10 : tensor<1x1x5x64x1x1x1xf32>) -> tensor<1x1x5x64x1x1x1xf32>
        %c0 = arith.constant 0 : index
        %c0_6 = arith.constant 0 : index
        %c0_7 = arith.constant 0 : index
        %c3 = arith.constant 3 : index
        %c3_8 = arith.constant 3 : index
        %c128 = arith.constant 128 : index
        %c1 = arith.constant 1 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %12 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %11) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0_6 to %c3_8 step %c1_9 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0_7 to %c128 step %c1_10 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_12 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_14 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<1x1x5x64x1x1x1xf32>
              %16 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_12, %extracted_slice_13 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%extracted_slice_14 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_15 fastmath<fast> : f32
                %18 = arith.addf %out, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              %inserted_slice = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %reduced = linalg.reduce ins(%12 : tensor<1x1x5x64x1x1x1xf32>) outs(%broadcasted : tensor<1x1x5x64xf32>) dimensions = [4, 5, 6] 
          (%in: f32, %init: f32) {
            %14 = arith.addf %in, %init fastmath<fast> : f32
            linalg.yield %14 : f32
          }
        %extracted_slice_11 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %13 = linalg.generic {indexing_maps = [#map5, #map6, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %reduced : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_11 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_12: f32, %out: f32):
          %14 = arith.maxnumf %in, %in_12 fastmath<fast> : f32
          linalg.yield %14 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before TutorialTileAndFuse (tutorial-tile-and-fuse) ('func.func' operation: @conv) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map5 = affine_map<(d0, d1, d2, d3) -> ()>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %broadcasted = linalg.broadcast ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) dimensions = [0, 1, 2] 
        %10 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %cst_5 = arith.constant 0.000000e+00 : f32
        %11 = linalg.fill ins(%cst_5 : f32) outs(%10 : tensor<1x1x5x64x1x1x1xf32>) -> tensor<1x1x5x64x1x1x1xf32>
        %c0 = arith.constant 0 : index
        %c0_6 = arith.constant 0 : index
        %c0_7 = arith.constant 0 : index
        %c3 = arith.constant 3 : index
        %c3_8 = arith.constant 3 : index
        %c128 = arith.constant 128 : index
        %c1 = arith.constant 1 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %12 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %11) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0_6 to %c3_8 step %c1_9 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0_7 to %c128 step %c1_10 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_12 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_14 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<1x1x5x64x1x1x1xf32>
              %16 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_12, %extracted_slice_13 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%extracted_slice_14 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_15 fastmath<fast> : f32
                %18 = arith.addf %out, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              %inserted_slice = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %reduced = linalg.reduce ins(%12 : tensor<1x1x5x64x1x1x1xf32>) outs(%broadcasted : tensor<1x1x5x64xf32>) dimensions = [4, 5, 6] 
          (%in: f32, %init: f32) {
            %14 = arith.addf %in, %init fastmath<fast> : f32
            linalg.yield %14 : f32
          }
        %extracted_slice_11 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %13 = linalg.generic {indexing_maps = [#map5, #map6, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %reduced : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_11 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_12: f32, %out: f32):
          %14 = arith.maxnumf %in, %in_12 fastmath<fast> : f32
          linalg.yield %14 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before TutorialTileAndFuse (tutorial-tile-and-fuse) ('func.func' operation: @conv) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map5 = affine_map<(d0, d1, d2, d3) -> ()>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %broadcasted = linalg.broadcast ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) dimensions = [0, 1, 2] 
        %10 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %cst_5 = arith.constant 0.000000e+00 : f32
        %11 = linalg.fill ins(%cst_5 : f32) outs(%10 : tensor<1x1x5x64x1x1x1xf32>) -> tensor<1x1x5x64x1x1x1xf32>
        %c0 = arith.constant 0 : index
        %c0_6 = arith.constant 0 : index
        %c0_7 = arith.constant 0 : index
        %c3 = arith.constant 3 : index
        %c3_8 = arith.constant 3 : index
        %c128 = arith.constant 128 : index
        %c1 = arith.constant 1 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %12 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %11) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0_6 to %c3_8 step %c1_9 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0_7 to %c128 step %c1_10 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_12 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_14 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<1x1x5x64x1x1x1xf32>
              %16 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_12, %extracted_slice_13 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%extracted_slice_14 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_15 fastmath<fast> : f32
                %18 = arith.addf %out, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              %inserted_slice = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %reduced = linalg.reduce ins(%12 : tensor<1x1x5x64x1x1x1xf32>) outs(%broadcasted : tensor<1x1x5x64xf32>) dimensions = [4, 5, 6] 
          (%in: f32, %init: f32) {
            %14 = arith.addf %in, %init fastmath<fast> : f32
            linalg.yield %14 : f32
          }
        %extracted_slice_11 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %13 = linalg.generic {indexing_maps = [#map5, #map6, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %reduced : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_11 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_12: f32, %out: f32):
          %14 = arith.maxnumf %in, %in_12 fastmath<fast> : f32
          linalg.yield %14 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before LinalgGeneralizeNamedOpsPass (linalg-generalize-named-ops) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map3 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map5 = affine_map<(d0, d1, d2, d3) -> ()>
#map6 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %broadcasted = linalg.broadcast ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) dimensions = [0, 1, 2] 
        %10 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %cst_5 = arith.constant 0.000000e+00 : f32
        %11 = linalg.fill ins(%cst_5 : f32) outs(%10 : tensor<1x1x5x64x1x1x1xf32>) -> tensor<1x1x5x64x1x1x1xf32>
        %c0 = arith.constant 0 : index
        %c0_6 = arith.constant 0 : index
        %c0_7 = arith.constant 0 : index
        %c3 = arith.constant 3 : index
        %c3_8 = arith.constant 3 : index
        %c128 = arith.constant 128 : index
        %c1 = arith.constant 1 : index
        %c1_9 = arith.constant 1 : index
        %c1_10 = arith.constant 1 : index
        %12 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %11) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0_6 to %c3_8 step %c1_9 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0_7 to %c128 step %c1_10 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_12 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_14 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<1x1x5x64x1x1x1xf32>
              %16 = linalg.generic {indexing_maps = [#map2, #map3, #map4], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_12, %extracted_slice_13 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%extracted_slice_14 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_15 fastmath<fast> : f32
                %18 = arith.addf %out, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              %inserted_slice = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %reduced = linalg.reduce ins(%12 : tensor<1x1x5x64x1x1x1xf32>) outs(%broadcasted : tensor<1x1x5x64xf32>) dimensions = [4, 5, 6] 
          (%in: f32, %init: f32) {
            %14 = arith.addf %in, %init fastmath<fast> : f32
            linalg.yield %14 : f32
          }
        %extracted_slice_11 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %13 = linalg.generic {indexing_maps = [#map5, #map6, #map6], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %reduced : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_11 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_12: f32, %out: f32):
          %14 = arith.maxnumf %in, %in_12 fastmath<fast> : f32
          linalg.yield %14 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After LinalgGeneralizeNamedOpsPass (linalg-generalize-named-ops) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %11 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %12 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%11 : tensor<1x1x5x64x1x1x1xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64x1x1x1xf32>
        %13 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %12) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %16 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %17 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %18 = linalg.generic {indexing_maps = [#map6, #map7, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %19 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %20 = arith.addf %out, %19 fastmath<fast> : f32
                linalg.yield %20 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %18 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %17 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %16 : tensor<1x1x5x64x1x1x1xf32>
        }
        %14 = linalg.generic {indexing_maps = [#map5, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%13 : tensor<1x1x5x64x1x1x1xf32>) outs(%10 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %16 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %15 = linalg.generic {indexing_maps = [#map9, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %14 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %16 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %11 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %12 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%11 : tensor<1x1x5x64x1x1x1xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64x1x1x1xf32>
        %13 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %12) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %16 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %17 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %18 = linalg.generic {indexing_maps = [#map6, #map7, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %19 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %20 = arith.addf %out, %19 fastmath<fast> : f32
                linalg.yield %20 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %18 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %17 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %16 : tensor<1x1x5x64x1x1x1xf32>
        }
        %14 = linalg.generic {indexing_maps = [#map5, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%13 : tensor<1x1x5x64x1x1x1xf32>) outs(%10 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %16 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %15 = linalg.generic {indexing_maps = [#map9, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %14 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %16 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %3 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %3] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %4 = affine.apply #map(%arg4)
      %5 = affine.apply #map(%arg4)
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %5] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %6 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %7 = affine.apply #map1(%arg8)
        %8 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %8, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %9 = affine.apply #map1(%arg8)
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %9, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %11 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %12 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%11 : tensor<1x1x5x64x1x1x1xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64x1x1x1xf32>
        %13 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %12) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %16 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %17 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %18 = linalg.generic {indexing_maps = [#map6, #map7, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %19 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %20 = arith.addf %out, %19 fastmath<fast> : f32
                linalg.yield %20 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %18 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %17 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %16 : tensor<1x1x5x64x1x1x1xf32>
        }
        %14 = linalg.generic {indexing_maps = [#map5, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%13 : tensor<1x1x5x64x1x1x1xf32>) outs(%10 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %16 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %15 = linalg.generic {indexing_maps = [#map9, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %14 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %16 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %16 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15 into %arg9[%arg6, %arg7, %7, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %6 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %2] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%2] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %3 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %4 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %4, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %5 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %6 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %7 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%6 : tensor<1x1x5x64x1x1x1xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64x1x1x1xf32>
        %8 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %7) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %11 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %12 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %13 = linalg.generic {indexing_maps = [#map6, #map7, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %14 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %15 = arith.addf %out, %14 fastmath<fast> : f32
                linalg.yield %15 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %12 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %11 : tensor<1x1x5x64x1x1x1xf32>
        }
        %9 = linalg.generic {indexing_maps = [#map5, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%8 : tensor<1x1x5x64x1x1x1xf32>) outs(%5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %11 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map9, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %9 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %11 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %3 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before LoopInvariantCodeMotion (loop-invariant-code-motion) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map3 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map4 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map5 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %2 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %2] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%2] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %3 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %4 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %4, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %5 = linalg.generic {indexing_maps = [#map2, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %6 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
        %7 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%6 : tensor<1x1x5x64x1x1x1xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64x1x1x1xf32>
        %8 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %7) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %11 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %12 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %13 = linalg.generic {indexing_maps = [#map6, #map7, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %14 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %15 = arith.addf %out, %14 fastmath<fast> : f32
                linalg.yield %15 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %12 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %11 : tensor<1x1x5x64x1x1x1xf32>
        }
        %9 = linalg.generic {indexing_maps = [#map5, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%8 : tensor<1x1x5x64x1x1x1xf32>) outs(%5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %11 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map9, #map3, #map3], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %9 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %11 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %4, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %3 into %arg5[0, 0, 0, %2] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %1 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After LoopInvariantCodeMotion (loop-invariant-code-motion) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map2 = affine_map<(d0) -> (d0 * 64)>
#map3 = affine_map<(d0) -> (d0 * 5)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%1 : tensor<1x1x5x64x1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map3(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %7 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %8 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %2) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %11 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %12 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %13 = linalg.generic {indexing_maps = [#map6, #map7, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %14 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %15 = arith.addf %out, %14 fastmath<fast> : f32
                linalg.yield %15 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %12 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %11 : tensor<1x1x5x64x1x1x1xf32>
        }
        %9 = linalg.generic {indexing_maps = [#map1, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%8 : tensor<1x1x5x64x1x1x1xf32>) outs(%7 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %11 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map9, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %9 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %11 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before LinalgFoldUnitExtentDimsPass (linalg-fold-unit-extent-dims) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> ()>
#map1 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3, d4, d5, d6)>
#map2 = affine_map<(d0) -> (d0 * 64)>
#map3 = affine_map<(d0) -> (d0 * 5)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d3)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
#map6 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d6, d4, d5, d3)>
#map7 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1 + d4, d2 + d5, d6)>
#map8 = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d0, d1, d2, d3)>
#map9 = affine_map<(d0, d1, d2, d3) -> ()>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<5x80x100x128xf32>
    %1 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%cst : f32) outs(%1 : tensor<1x1x5x64x1x1x1xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %0[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map3(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %extracted_slice_4 = tensor.extract_slice %extracted_slice_1[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %7 = linalg.generic {indexing_maps = [#map4, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%extracted_slice_4 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<1x1x5x64xf32>
        %8 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %2) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %11 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %12 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_6 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_7 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %13 = linalg.generic {indexing_maps = [#map6, #map7, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel", "parallel", "parallel", "parallel"]} ins(%extracted_slice_6, %extracted_slice_7 : tensor<1x1x1x64xf32>, tensor<1x1x5x1xf32>) outs(%arg15 : tensor<1x1x5x64x1x1x1xf32>) {
              ^bb0(%in: f32, %in_8: f32, %out: f32):
                %14 = arith.mulf %in, %in_8 fastmath<fast> : f32
                %15 = arith.addf %out, %14 fastmath<fast> : f32
                linalg.yield %15 : f32
              } -> tensor<1x1x5x64x1x1x1xf32>
              scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %12 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %11 : tensor<1x1x5x64x1x1x1xf32>
        }
        %9 = linalg.generic {indexing_maps = [#map1, #map8], iterator_types = ["parallel", "parallel", "parallel", "parallel", "reduction", "reduction", "reduction"]} ins(%8 : tensor<1x1x5x64x1x1x1xf32>) outs(%7 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          %11 = arith.addf %in, %out fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %10 = linalg.generic {indexing_maps = [#map9, #map5, #map5], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%cst, %9 : f32, tensor<1x1x5x64xf32>) outs(%extracted_slice_5 : tensor<1x1x5x64xf32>) {
        ^bb0(%in: f32, %in_6: f32, %out: f32):
          %11 = arith.maxnumf %in, %in_6 fastmath<fast> : f32
          linalg.yield %11 : f32
        } -> tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After LinalgFoldUnitExtentDimsPass (linalg-fold-unit-extent-dims) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1) -> ()>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
#map2 = affine_map<(d0) -> (d0 * 64)>
#map3 = affine_map<(d0) -> (d0 * 5)>
#map4 = affine_map<(d0, d1) -> (d1)>
#map5 = affine_map<(d0, d1) -> (d0)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%cst : f32) outs(%1 : tensor<5x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_1) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map3(%arg8)
        %extracted_slice_2 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = tensor.empty() : tensor<5x64xf32>
        %8 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%7 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %13 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %14 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_7 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_8 = tensor.extract_slice %extracted_slice_2[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_7[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_11 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %15 = tensor.empty() : tensor<5x64xf32>
              %extracted_slice_12 = tensor.extract_slice %extracted_slice_10[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %16 = linalg.generic {indexing_maps = [#map4, #map5, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_9, %extracted_slice_12, %extracted_slice_11 : tensor<64xf32>, tensor<5xf32>, tensor<5x64xf32>) outs(%15 : tensor<5x64xf32>) {
              ^bb0(%in: f32, %in_14: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_14 fastmath<fast> : f32
                %18 = arith.addf %in_15, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<5x64xf32>
              %inserted_slice_13 = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_3 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = tensor.empty() : tensor<5x64xf32>
        %11 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_3, %8 : tensor<5x64xf32>, tensor<5x64xf32>) outs(%10 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %in_7: f32, %out: f32):
          %13 = arith.addf %in, %in_7 fastmath<fast> : f32
          linalg.yield %13 : f32
        } -> tensor<5x64xf32>
        %extracted_slice_4 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %extracted_slice_4[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%cst, %11 : f32, tensor<5x64xf32>) outs(%extracted_slice_5 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %in_7: f32, %out: f32):
          %13 = arith.maxnumf %in, %in_7 fastmath<fast> : f32
          linalg.yield %13 : f32
        } -> tensor<5x64xf32>
        %inserted_slice_6 = tensor.insert_slice %12 into %extracted_slice_4[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_6 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before TutorialVectorization (tutorial-vectorization) ('func.func' operation: @conv) //----- //
#map = affine_map<(d0, d1) -> ()>
#map1 = affine_map<(d0, d1) -> (d0, d1)>
#map2 = affine_map<(d0) -> (d0 * 64)>
#map3 = affine_map<(d0) -> (d0 * 5)>
#map4 = affine_map<(d0, d1) -> (d1)>
#map5 = affine_map<(d0, d1) -> (d0)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel"]} ins(%cst : f32) outs(%1 : tensor<5x64xf32>) {
    ^bb0(%in: f32, %out: f32):
      linalg.yield %in : f32
    } -> tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map2(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_0 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_1) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map3(%arg8)
        %extracted_slice_2 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = tensor.empty() : tensor<5x64xf32>
        %8 = linalg.generic {indexing_maps = [#map4, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_0 : tensor<64xf32>) outs(%7 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %out: f32):
          linalg.yield %in : f32
        } -> tensor<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %13 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %14 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_7 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_8 = tensor.extract_slice %extracted_slice_2[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_7[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_11 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %15 = tensor.empty() : tensor<5x64xf32>
              %extracted_slice_12 = tensor.extract_slice %extracted_slice_10[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %16 = linalg.generic {indexing_maps = [#map4, #map5, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_9, %extracted_slice_12, %extracted_slice_11 : tensor<64xf32>, tensor<5xf32>, tensor<5x64xf32>) outs(%15 : tensor<5x64xf32>) {
              ^bb0(%in: f32, %in_14: f32, %in_15: f32, %out: f32):
                %17 = arith.mulf %in, %in_14 fastmath<fast> : f32
                %18 = arith.addf %in_15, %17 fastmath<fast> : f32
                linalg.yield %18 : f32
              } -> tensor<5x64xf32>
              %inserted_slice_13 = tensor.insert_slice %16 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_13 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %13 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_3 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = tensor.empty() : tensor<5x64xf32>
        %11 = linalg.generic {indexing_maps = [#map1, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%extracted_slice_3, %8 : tensor<5x64xf32>, tensor<5x64xf32>) outs(%10 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %in_7: f32, %out: f32):
          %13 = arith.addf %in, %in_7 fastmath<fast> : f32
          linalg.yield %13 : f32
        } -> tensor<5x64xf32>
        %extracted_slice_4 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %extracted_slice_4[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = linalg.generic {indexing_maps = [#map, #map1, #map1], iterator_types = ["parallel", "parallel"]} ins(%cst, %11 : f32, tensor<5x64xf32>) outs(%extracted_slice_5 : tensor<5x64xf32>) {
        ^bb0(%in: f32, %in_7: f32, %out: f32):
          %13 = arith.maxnumf %in, %in_7 fastmath<fast> : f32
          linalg.yield %13 : f32
        } -> tensor<5x64xf32>
        %inserted_slice_6 = tensor.insert_slice %12 into %extracted_slice_4[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_6 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After TutorialVectorization (tutorial-vectorization) ('func.func' operation: @conv) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = vector.transfer_write %cst, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = vector.transfer_read %extracted_slice_1[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_11 = tensor.extract_slice %extracted_slice_9[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_12 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %16 = tensor.empty() : tensor<5x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_11[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %17 = vector.transfer_read %extracted_slice_10[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
              %18 = vector.broadcast %17 : vector<64xf32> to vector<5x64xf32>
              %19 = vector.transfer_read %extracted_slice_13[%c0], %cst_0 {in_bounds = [true]} : tensor<5xf32>, vector<5xf32>
              %20 = vector.broadcast %19 : vector<5xf32> to vector<64x5xf32>
              %21 = vector.transpose %20, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %22 = vector.transfer_read %extracted_slice_12[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
              %23 = arith.mulf %18, %21 fastmath<fast> : vector<5x64xf32>
              %24 = arith.addf %22, %23 fastmath<fast> : vector<5x64xf32>
              %25 = vector.transfer_write %24, %16[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
              %inserted_slice_14 = tensor.insert_slice %25 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_14 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_4 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = vector.transfer_read %extracted_slice_4[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_6[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %inserted_slice_7 = tensor.insert_slice %13 into %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_7 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = vector.transfer_write %cst, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = vector.transfer_read %extracted_slice_1[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_11 = tensor.extract_slice %extracted_slice_9[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_12 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %16 = tensor.empty() : tensor<5x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_11[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %17 = vector.transfer_read %extracted_slice_10[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
              %18 = vector.broadcast %17 : vector<64xf32> to vector<5x64xf32>
              %19 = vector.transfer_read %extracted_slice_13[%c0], %cst_0 {in_bounds = [true]} : tensor<5xf32>, vector<5xf32>
              %20 = vector.broadcast %19 : vector<5xf32> to vector<64x5xf32>
              %21 = vector.transpose %20, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %22 = vector.transfer_read %extracted_slice_12[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
              %23 = arith.mulf %18, %21 fastmath<fast> : vector<5x64xf32>
              %24 = arith.addf %22, %23 fastmath<fast> : vector<5x64xf32>
              %25 = vector.transfer_write %24, %16[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
              %inserted_slice_14 = tensor.insert_slice %25 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_14 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_4 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = vector.transfer_read %extracted_slice_4[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_6[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %inserted_slice_7 = tensor.insert_slice %13 into %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_7 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = vector.transfer_write %cst, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = vector.transfer_read %extracted_slice_1[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_11 = tensor.extract_slice %extracted_slice_9[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_12 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %16 = tensor.empty() : tensor<5x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_11[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %17 = vector.transfer_read %extracted_slice_10[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
              %18 = vector.broadcast %17 : vector<64xf32> to vector<5x64xf32>
              %19 = vector.transfer_read %extracted_slice_13[%c0], %cst_0 {in_bounds = [true]} : tensor<5xf32>, vector<5xf32>
              %20 = vector.broadcast %19 : vector<5xf32> to vector<64x5xf32>
              %21 = vector.transpose %20, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %22 = vector.transfer_read %extracted_slice_12[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
              %23 = arith.mulf %18, %21 fastmath<fast> : vector<5x64xf32>
              %24 = arith.addf %22, %23 fastmath<fast> : vector<5x64xf32>
              %25 = vector.transfer_write %24, %16[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
              %inserted_slice_14 = tensor.insert_slice %25 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_14 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_4 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = vector.transfer_read %extracted_slice_4[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_6[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %inserted_slice_7 = tensor.insert_slice %13 into %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_7 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = vector.transfer_write %cst, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = vector.transfer_read %extracted_slice_1[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_11 = tensor.extract_slice %extracted_slice_9[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_12 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_11[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %16 = vector.transfer_read %extracted_slice_10[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
              %17 = vector.broadcast %16 : vector<64xf32> to vector<5x64xf32>
              %18 = vector.transfer_read %extracted_slice_13[%c0], %cst_0 {in_bounds = [true]} : tensor<5xf32>, vector<5xf32>
              %19 = vector.broadcast %18 : vector<5xf32> to vector<64x5xf32>
              %20 = vector.transpose %19, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %21 = vector.transfer_read %extracted_slice_12[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
              %22 = arith.mulf %17, %20 fastmath<fast> : vector<5x64xf32>
              %23 = arith.addf %21, %22 fastmath<fast> : vector<5x64xf32>
              %24 = vector.transfer_write %23, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
              %inserted_slice_14 = tensor.insert_slice %24 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_14 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_4 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = vector.transfer_read %extracted_slice_4[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_6[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %inserted_slice_7 = tensor.insert_slice %13 into %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_7 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before FoldTensorSubsetOps (fold-tensor-subset-ops) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = tensor.empty() : tensor<5x64xf32>
    %2 = vector.transfer_write %cst, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
    %inserted_slice = tensor.insert_slice %2 into %0[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
    %3 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %4 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg1[0, 0, 0, %4] [128, 3, 3, 64] [1, 1, 1, 1] : tensor<128x3x3x128xf32> to tensor<128x3x3x64xf32>
      %extracted_slice_1 = tensor.extract_slice %arg2[%4] [64] [1] : tensor<128xf32> to tensor<64xf32>
      %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %5 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice_2) -> (tensor<5x80x100x64xf32>) {
        %6 = affine.apply #map1(%arg8)
        %extracted_slice_3 = tensor.extract_slice %arg0[%arg6, %arg7, %6, 0] [1, 3, 7, 128] [1, 1, 1, 1] : tensor<5x82x102x128xf32> to tensor<1x3x7x128xf32>
        %7 = vector.transfer_read %extracted_slice_1[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %inserted_slice) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %14 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %15 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg14, %arg10, %arg12, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<128x3x3x64xf32> to tensor<1x1x1x64xf32>
              %extracted_slice_9 = tensor.extract_slice %extracted_slice_3[0, %arg10, %arg12, %arg14] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x3x7x128xf32> to tensor<1x1x5x1xf32>
              %extracted_slice_10 = tensor.extract_slice %extracted_slice_8[0, 0, 0, 0] [1, 1, 1, 64] [1, 1, 1, 1] : tensor<1x1x1x64xf32> to tensor<64xf32>
              %extracted_slice_11 = tensor.extract_slice %extracted_slice_9[0, 0, 0, 0] [1, 1, 5, 1] [1, 1, 1, 1] : tensor<1x1x5x1xf32> to tensor<1x5xf32>
              %extracted_slice_12 = tensor.extract_slice %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
              %extracted_slice_13 = tensor.extract_slice %extracted_slice_11[0, 0] [1, 5] [1, 1] : tensor<1x5xf32> to tensor<5xf32>
              %16 = vector.transfer_read %extracted_slice_10[%c0], %cst_0 {in_bounds = [true]} : tensor<64xf32>, vector<64xf32>
              %17 = vector.broadcast %16 : vector<64xf32> to vector<5x64xf32>
              %18 = vector.transfer_read %extracted_slice_13[%c0], %cst_0 {in_bounds = [true]} : tensor<5xf32>, vector<5xf32>
              %19 = vector.broadcast %18 : vector<5xf32> to vector<64x5xf32>
              %20 = vector.transpose %19, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %21 = vector.transfer_read %extracted_slice_12[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
              %22 = arith.mulf %17, %20 fastmath<fast> : vector<5x64xf32>
              %23 = arith.addf %21, %22 fastmath<fast> : vector<5x64xf32>
              %24 = vector.transfer_write %23, %1[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
              %inserted_slice_14 = tensor.insert_slice %24 into %arg15[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64x1x1x1xf32>
              scf.yield %inserted_slice_14 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %14 : tensor<1x1x5x64x1x1x1xf32>
        }
        %extracted_slice_4 = tensor.extract_slice %9[0, 0, 0, 0, 0, 0, 0] [1, 1, 5, 64, 1, 1, 1] [1, 1, 1, 1, 1, 1, 1] : tensor<1x1x5x64x1x1x1xf32> to tensor<5x64xf32>
        %10 = vector.transfer_read %extracted_slice_4[%c0, %c0], %cst_0 {in_bounds = [true, true]} : tensor<5x64xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_5 = tensor.extract_slice %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_6 = tensor.extract_slice %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_6[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %inserted_slice_7 = tensor.insert_slice %13 into %extracted_slice_5[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<1x1x5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %inserted_slice_7 into %arg9[%arg6, %arg7, %6, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %5 into %arg5[0, 0, 0, %4] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %3 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After FoldTensorSubsetOps (fold-tensor-subset-ops) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d2, d3)>
#map1 = affine_map<(d0) -> (d0 * 64)>
#map2 = affine_map<(d0) -> (d0 * 5)>
#map3 = affine_map<()[s0, s1] -> (s0 + s1)>
#map4 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = vector.transfer_write %cst, %0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
    %2 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %3 = affine.apply #map1(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %4 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %5 = affine.apply #map2(%arg8)
        %6 = affine.apply #map1(%arg4)
        %7 = vector.transfer_read %arg2[%6], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %1) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %15 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %16 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %17 = affine.apply #map1(%arg4)
              %18 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %17], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %19 = vector.broadcast %18 : vector<64xf32> to vector<5x64xf32>
              %20 = affine.apply #map3()[%arg7, %arg10]
              %21 = affine.apply #map4(%arg8)[%arg12]
              %22 = vector.transfer_read %arg0[%arg6, %20, %21, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map5} : tensor<5x82x102x128xf32>, vector<5xf32>
              %23 = vector.broadcast %22 : vector<5xf32> to vector<64x5xf32>
              %24 = vector.transpose %23, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %25 = vector.transfer_read %arg15[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
              %26 = arith.mulf %19, %24 fastmath<fast> : vector<5x64xf32>
              %27 = arith.addf %25, %26 fastmath<fast> : vector<5x64xf32>
              %28 = vector.transfer_write %27, %arg15[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
              scf.yield %28 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %16 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
        }
        %10 = vector.transfer_read %9[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %5, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %14 = affine.apply #map2(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %14, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %4 into %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before LoopInvariantSubsetHoisting (loop-invariant-subset-hoisting) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d2, d3)>
#map1 = affine_map<(d0) -> (d0 * 64)>
#map2 = affine_map<(d0) -> (d0 * 5)>
#map3 = affine_map<()[s0, s1] -> (s0 + s1)>
#map4 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = vector.transfer_write %cst, %0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
    %2 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %3 = affine.apply #map1(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %4 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %5 = affine.apply #map2(%arg8)
        %6 = affine.apply #map1(%arg4)
        %7 = vector.transfer_read %arg2[%6], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %1) -> (tensor<1x1x5x64x1x1x1xf32>) {
          %15 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (tensor<1x1x5x64x1x1x1xf32>) {
            %16 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (tensor<1x1x5x64x1x1x1xf32>) {
              %17 = affine.apply #map1(%arg4)
              %18 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %17], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %19 = vector.broadcast %18 : vector<64xf32> to vector<5x64xf32>
              %20 = affine.apply #map3()[%arg7, %arg10]
              %21 = affine.apply #map4(%arg8)[%arg12]
              %22 = vector.transfer_read %arg0[%arg6, %20, %21, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map5} : tensor<5x82x102x128xf32>, vector<5xf32>
              %23 = vector.broadcast %22 : vector<5xf32> to vector<64x5xf32>
              %24 = vector.transpose %23, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %25 = vector.transfer_read %arg15[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
              %26 = arith.mulf %19, %24 fastmath<fast> : vector<5x64xf32>
              %27 = arith.addf %25, %26 fastmath<fast> : vector<5x64xf32>
              %28 = vector.transfer_write %27, %arg15[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
              scf.yield %28 : tensor<1x1x5x64x1x1x1xf32>
            }
            scf.yield %16 : tensor<1x1x5x64x1x1x1xf32>
          }
          scf.yield %15 : tensor<1x1x5x64x1x1x1xf32>
        }
        %10 = vector.transfer_read %9[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %11 = arith.addf %10, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %5, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %12 = arith.maxnumf %11, %cst fastmath<fast> : vector<5x64xf32>
        %13 = vector.transfer_write %12, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %14 = affine.apply #map2(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %13 into %arg9[%arg6, %arg7, %14, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %4 into %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After LoopInvariantSubsetHoisting (loop-invariant-subset-hoisting) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d2, d3)>
#map1 = affine_map<(d0) -> (d0 * 64)>
#map2 = affine_map<(d0) -> (d0 * 5)>
#map3 = affine_map<()[s0, s1] -> (s0 + s1)>
#map4 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = vector.transfer_write %cst, %0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
    %2 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %3 = affine.apply #map1(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %4 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %5 = affine.apply #map2(%arg8)
        %6 = affine.apply #map1(%arg4)
        %7 = vector.transfer_read %arg2[%6], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = vector.transfer_read %1[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %10:2 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %1, %arg12 = %9) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
          %17:2 = scf.for %arg13 = %c0 to %c3 step %c1 iter_args(%arg14 = %arg11, %arg15 = %arg12) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
            %18:2 = scf.for %arg16 = %c0 to %c128 step %c1 iter_args(%arg17 = %arg14, %arg18 = %arg15) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
              %19 = affine.apply #map1(%arg4)
              %20 = vector.transfer_read %arg1[%arg16, %arg10, %arg13, %19], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %21 = vector.broadcast %20 : vector<64xf32> to vector<5x64xf32>
              %22 = affine.apply #map3()[%arg7, %arg10]
              %23 = affine.apply #map4(%arg8)[%arg13]
              %24 = vector.transfer_read %arg0[%arg6, %22, %23, %arg16], %cst_0 {in_bounds = [true], permutation_map = #map5} : tensor<5x82x102x128xf32>, vector<5xf32>
              %25 = vector.broadcast %24 : vector<5xf32> to vector<64x5xf32>
              %26 = vector.transpose %25, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %27 = arith.mulf %21, %26 fastmath<fast> : vector<5x64xf32>
              %28 = arith.addf %arg18, %27 fastmath<fast> : vector<5x64xf32>
              scf.yield %arg17, %28 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
            }
            scf.yield %18#0, %18#1 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
          }
          scf.yield %17#0, %17#1 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        }
        %11 = vector.transfer_write %10#1, %10#0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
        %12 = vector.transfer_read %11[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %13 = arith.addf %12, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %5, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %14 = arith.maxnumf %13, %cst fastmath<fast> : vector<5x64xf32>
        %15 = vector.transfer_write %14, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %16 = affine.apply #map2(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15 into %arg9[%arg6, %arg7, %16, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %4 into %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0, d1, d2, d3, d4, d5, d6) -> (d2, d3)>
#map1 = affine_map<(d0) -> (d0 * 64)>
#map2 = affine_map<(d0) -> (d0 * 5)>
#map3 = affine_map<()[s0, s1] -> (s0 + s1)>
#map4 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map5 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = tensor.empty() : tensor<1x1x5x64x1x1x1xf32>
    %1 = vector.transfer_write %cst, %0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
    %2 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %3 = affine.apply #map1(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %4 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %5 = affine.apply #map2(%arg8)
        %6 = affine.apply #map1(%arg4)
        %7 = vector.transfer_read %arg2[%6], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %8 = vector.broadcast %7 : vector<64xf32> to vector<5x64xf32>
        %9 = vector.transfer_read %1[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %10:2 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %1, %arg12 = %9) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
          %17:2 = scf.for %arg13 = %c0 to %c3 step %c1 iter_args(%arg14 = %arg11, %arg15 = %arg12) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
            %18:2 = scf.for %arg16 = %c0 to %c128 step %c1 iter_args(%arg17 = %arg14, %arg18 = %arg15) -> (tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>) {
              %19 = affine.apply #map1(%arg4)
              %20 = vector.transfer_read %arg1[%arg16, %arg10, %arg13, %19], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %21 = vector.broadcast %20 : vector<64xf32> to vector<5x64xf32>
              %22 = affine.apply #map3()[%arg7, %arg10]
              %23 = affine.apply #map4(%arg8)[%arg13]
              %24 = vector.transfer_read %arg0[%arg6, %22, %23, %arg16], %cst_0 {in_bounds = [true], permutation_map = #map5} : tensor<5x82x102x128xf32>, vector<5xf32>
              %25 = vector.broadcast %24 : vector<5xf32> to vector<64x5xf32>
              %26 = vector.transpose %25, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %27 = arith.mulf %21, %26 fastmath<fast> : vector<5x64xf32>
              %28 = arith.addf %arg18, %27 fastmath<fast> : vector<5x64xf32>
              scf.yield %arg17, %28 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
            }
            scf.yield %18#0, %18#1 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
          }
          scf.yield %17#0, %17#1 : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        }
        %11 = vector.transfer_write %10#1, %10#0[%c0, %c0, %c0, %c0, %c0, %c0, %c0] {in_bounds = [true, true], permutation_map = #map} : vector<5x64xf32>, tensor<1x1x5x64x1x1x1xf32>
        %12 = vector.transfer_read %11[%c0, %c0, %c0, %c0, %c0, %c0, %c0], %cst_0 {in_bounds = [true, true], permutation_map = #map} : tensor<1x1x5x64x1x1x1xf32>, vector<5x64xf32>
        %13 = arith.addf %12, %8 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %5, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %14 = arith.maxnumf %13, %cst fastmath<fast> : vector<5x64xf32>
        %15 = vector.transfer_write %14, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %16 = affine.apply #map2(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15 into %arg9[%arg6, %arg7, %16, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %4 into %arg5[0, 0, 0, %3] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %2 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<()[s0, s1] -> (s0 + s1)>
#map3 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %1 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %2 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %3 = affine.apply #map1(%arg8)
        %4 = affine.apply #map(%arg4)
        %5 = vector.transfer_read %arg2[%4], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %6 = vector.broadcast %5 : vector<64xf32> to vector<5x64xf32>
        %7 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %cst) -> (vector<5x64xf32>) {
          %12 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (vector<5x64xf32>) {
            %13 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (vector<5x64xf32>) {
              %14 = affine.apply #map(%arg4)
              %15 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %14], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %16 = vector.broadcast %15 : vector<64xf32> to vector<5x64xf32>
              %17 = affine.apply #map2()[%arg7, %arg10]
              %18 = affine.apply #map3(%arg8)[%arg12]
              %19 = vector.transfer_read %arg0[%arg6, %17, %18, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map4} : tensor<5x82x102x128xf32>, vector<5xf32>
              %20 = vector.broadcast %19 : vector<5xf32> to vector<64x5xf32>
              %21 = vector.transpose %20, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %22 = arith.mulf %16, %21 fastmath<fast> : vector<5x64xf32>
              %23 = arith.addf %arg15, %22 fastmath<fast> : vector<5x64xf32>
              scf.yield %23 : vector<5x64xf32>
            }
            scf.yield %13 : vector<5x64xf32>
          }
          scf.yield %12 : vector<5x64xf32>
        }
        %8 = arith.addf %7, %6 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %9 = arith.maxnumf %8, %cst fastmath<fast> : vector<5x64xf32>
        %10 = vector.transfer_write %9, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %11 = affine.apply #map1(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %11, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %2 into %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %0 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<()[s0, s1] -> (s0 + s1)>
#map3 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %1 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %2 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %3 = affine.apply #map1(%arg8)
        %4 = affine.apply #map(%arg4)
        %5 = vector.transfer_read %arg2[%4], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %6 = vector.broadcast %5 : vector<64xf32> to vector<5x64xf32>
        %7 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %cst) -> (vector<5x64xf32>) {
          %12 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (vector<5x64xf32>) {
            %13 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (vector<5x64xf32>) {
              %14 = affine.apply #map(%arg4)
              %15 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %14], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %16 = vector.broadcast %15 : vector<64xf32> to vector<5x64xf32>
              %17 = affine.apply #map2()[%arg7, %arg10]
              %18 = affine.apply #map3(%arg8)[%arg12]
              %19 = vector.transfer_read %arg0[%arg6, %17, %18, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map4} : tensor<5x82x102x128xf32>, vector<5xf32>
              %20 = vector.broadcast %19 : vector<5xf32> to vector<64x5xf32>
              %21 = vector.transpose %20, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %22 = arith.mulf %16, %21 fastmath<fast> : vector<5x64xf32>
              %23 = arith.addf %arg15, %22 fastmath<fast> : vector<5x64xf32>
              scf.yield %23 : vector<5x64xf32>
            }
            scf.yield %13 : vector<5x64xf32>
          }
          scf.yield %12 : vector<5x64xf32>
        }
        %8 = arith.addf %7, %6 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %9 = arith.maxnumf %8, %cst fastmath<fast> : vector<5x64xf32>
        %10 = vector.transfer_write %9, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        %11 = affine.apply #map1(%arg8)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %10 into %arg9[%arg6, %arg7, %11, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %2 into %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %0 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump After CSE (cse) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<()[s0, s1] -> (s0 + s1)>
#map3 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %1 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %2 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %3 = affine.apply #map1(%arg8)
        %4 = vector.transfer_read %arg2[%1], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %5 = vector.broadcast %4 : vector<64xf32> to vector<5x64xf32>
        %6 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %cst) -> (vector<5x64xf32>) {
          %10 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (vector<5x64xf32>) {
            %11 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (vector<5x64xf32>) {
              %12 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %1], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %13 = vector.broadcast %12 : vector<64xf32> to vector<5x64xf32>
              %14 = affine.apply #map2()[%arg7, %arg10]
              %15 = affine.apply #map3(%arg8)[%arg12]
              %16 = vector.transfer_read %arg0[%arg6, %14, %15, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map4} : tensor<5x82x102x128xf32>, vector<5xf32>
              %17 = vector.broadcast %16 : vector<5xf32> to vector<64x5xf32>
              %18 = vector.transpose %17, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %19 = arith.mulf %13, %18 fastmath<fast> : vector<5x64xf32>
              %20 = arith.addf %arg15, %19 fastmath<fast> : vector<5x64xf32>
              scf.yield %20 : vector<5x64xf32>
            }
            scf.yield %11 : vector<5x64xf32>
          }
          scf.yield %10 : vector<5x64xf32>
        }
        %7 = arith.addf %6, %5 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %8 = arith.maxnumf %7, %cst fastmath<fast> : vector<5x64xf32>
        %9 = vector.transfer_write %8, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %9 into %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %2 into %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %0 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


// -----// IR Dump Before OneShotBufferize (one-shot-bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<()[s0, s1] -> (s0 + s1)>
#map3 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %1 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %2 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %3 = affine.apply #map1(%arg8)
        %4 = vector.transfer_read %arg2[%1], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %5 = vector.broadcast %4 : vector<64xf32> to vector<5x64xf32>
        %6 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %cst) -> (vector<5x64xf32>) {
          %10 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (vector<5x64xf32>) {
            %11 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (vector<5x64xf32>) {
              %12 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %1], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %13 = vector.broadcast %12 : vector<64xf32> to vector<5x64xf32>
              %14 = affine.apply #map2()[%arg7, %arg10]
              %15 = affine.apply #map3(%arg8)[%arg12]
              %16 = vector.transfer_read %arg0[%arg6, %14, %15, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map4} : tensor<5x82x102x128xf32>, vector<5xf32>
              %17 = vector.broadcast %16 : vector<5xf32> to vector<64x5xf32>
              %18 = vector.transpose %17, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %19 = arith.mulf %13, %18 fastmath<fast> : vector<5x64xf32>
              %20 = arith.addf %arg15, %19 fastmath<fast> : vector<5x64xf32>
              scf.yield %20 : vector<5x64xf32>
            }
            scf.yield %11 : vector<5x64xf32>
          }
          scf.yield %10 : vector<5x64xf32>
        }
        %7 = arith.addf %6, %5 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %8 = arith.maxnumf %7, %cst fastmath<fast> : vector<5x64xf32>
        %9 = vector.transfer_write %8, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %9 into %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %2 into %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %0 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


example-transform.mlir:58:3: error: cannot bufferize a FuncOp with tensors and without a unique ReturnOp
  transform.named_sequence @__halide(
  ^
example-transform.mlir:58:3: note: see current operation: 
"transform.named_sequence"() <{function_type = (!transform.any_op) -> (), sym_name = "__halide"}> ({
^bb0(%arg0: !transform.any_op):
  %0 = "transform.structured.match"(%arg0) <{ops = ["linalg.broadcast"]}> : (!transform.any_op) -> !transform.any_op
  %1 = "transform.structured.match"(%arg0) <{ops = ["linalg.generic"]}> : (!transform.any_op) -> !transform.any_op
  %2:2 = "transform.split_handle"(%1) <{fail_on_payload_too_small = true, pass_through_empty_handle = true}> : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
  %3:2 = "transform.structured.tile_using_forall"(%2#1) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_tile_sizes = array<i64: 0, 0, 0, 64>}> : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
  %4:2 = "transform.structured.tile_using_forall"(%3#0) <{operandSegmentSizes = array<i32: 1, 0, 0, 0, 0>, static_tile_sizes = array<i64: 1, 1, 5, 0>}> : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
  %5:2 = "transform.structured.fuse_into_containing_op"(%2#0, %3#1) : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
  %6:2 = "transform.structured.fuse_into_containing_op"(%5#0, %4#1) : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
  %7:2 = "transform.structured.fuse_into_containing_op"(%0, %5#1) : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
  %8:2 = "transform.structured.fuse_into_containing_op"(%7#0, %6#1) : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
  %9 = "transform.structured.match"(%arg0) <{ops = ["func.func"]}> : (!transform.any_op) -> !transform.any_op
  "transform.apply_patterns"(%9) <{max_iterations = -1 : i64, max_num_rewrites = -1 : i64}> ({
  ^bb0:
  }) : (!transform.any_op) -> ()
  %10:4 = "transform.structured.tile_reduction_using_for"(%6#0) <{tile_sizes = array<i64: 0, 0, 0, 0, 1, 1, 1>}> : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
  "transform.yield"() : () -> ()
}) : () -> ()
// -----// IR Dump After OneShotBufferize Failed (one-shot-bufferize) ('builtin.module' operation) //----- //
#map = affine_map<(d0) -> (d0 * 64)>
#map1 = affine_map<(d0) -> (d0 * 5)>
#map2 = affine_map<()[s0, s1] -> (s0 + s1)>
#map3 = affine_map<(d0)[s0] -> (d0 * 5 + s0)>
#map4 = affine_map<(d0, d1, d2, d3) -> (d2)>
module attributes {transform.with_named_sequence} {
  func.func @conv(%arg0: tensor<5x82x102x128xf32>, %arg1: tensor<128x3x3x128xf32>, %arg2: tensor<128xf32>, %arg3: tensor<5x80x100x128xf32>) -> tensor<5x80x100x128xf32> attributes {transform_tiling_spec = "__halide"} {
    %cst = arith.constant dense<0.000000e+00> : vector<5x64xf32>
    %c1 = arith.constant 1 : index
    %c128 = arith.constant 128 : index
    %c3 = arith.constant 3 : index
    %c0 = arith.constant 0 : index
    %cst_0 = arith.constant 0.000000e+00 : f32
    %0 = scf.forall (%arg4) in (2) shared_outs(%arg5 = %arg3) -> (tensor<5x80x100x128xf32>) {
      %1 = affine.apply #map(%arg4)
      %extracted_slice = tensor.extract_slice %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x128xf32> to tensor<5x80x100x64xf32>
      %2 = scf.forall (%arg6, %arg7, %arg8) in (5, 80, 20) shared_outs(%arg9 = %extracted_slice) -> (tensor<5x80x100x64xf32>) {
        %3 = affine.apply #map1(%arg8)
        %4 = vector.transfer_read %arg2[%1], %cst_0 {in_bounds = [true]} : tensor<128xf32>, vector<64xf32>
        %5 = vector.broadcast %4 : vector<64xf32> to vector<5x64xf32>
        %6 = scf.for %arg10 = %c0 to %c3 step %c1 iter_args(%arg11 = %cst) -> (vector<5x64xf32>) {
          %10 = scf.for %arg12 = %c0 to %c3 step %c1 iter_args(%arg13 = %arg11) -> (vector<5x64xf32>) {
            %11 = scf.for %arg14 = %c0 to %c128 step %c1 iter_args(%arg15 = %arg13) -> (vector<5x64xf32>) {
              %12 = vector.transfer_read %arg1[%arg14, %arg10, %arg12, %1], %cst_0 {in_bounds = [true]} : tensor<128x3x3x128xf32>, vector<64xf32>
              %13 = vector.broadcast %12 : vector<64xf32> to vector<5x64xf32>
              %14 = affine.apply #map2()[%arg7, %arg10]
              %15 = affine.apply #map3(%arg8)[%arg12]
              %16 = vector.transfer_read %arg0[%arg6, %14, %15, %arg14], %cst_0 {in_bounds = [true], permutation_map = #map4} : tensor<5x82x102x128xf32>, vector<5xf32>
              %17 = vector.broadcast %16 : vector<5xf32> to vector<64x5xf32>
              %18 = vector.transpose %17, [1, 0] : vector<64x5xf32> to vector<5x64xf32>
              %19 = arith.mulf %13, %18 fastmath<fast> : vector<5x64xf32>
              %20 = arith.addf %arg15, %19 fastmath<fast> : vector<5x64xf32>
              scf.yield %20 : vector<5x64xf32>
            }
            scf.yield %11 : vector<5x64xf32>
          }
          scf.yield %10 : vector<5x64xf32>
        }
        %7 = arith.addf %6, %5 fastmath<fast> : vector<5x64xf32>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> to tensor<1x1x5x64xf32>
        %extracted_slice_2 = tensor.extract_slice %extracted_slice_1[0, 0, 0, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<1x1x5x64xf32> to tensor<5x64xf32>
        %8 = arith.maxnumf %7, %cst fastmath<fast> : vector<5x64xf32>
        %9 = vector.transfer_write %8, %extracted_slice_2[%c0, %c0] {in_bounds = [true, true]} : vector<5x64xf32>, tensor<5x64xf32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %9 into %arg9[%arg6, %arg7, %3, 0] [1, 1, 5, 64] [1, 1, 1, 1] : tensor<5x64xf32> into tensor<5x80x100x64xf32>
        }
      }
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %2 into %arg5[0, 0, 0, %1] [5, 80, 100, 64] [1, 1, 1, 1] : tensor<5x80x100x64xf32> into tensor<5x80x100x128xf32>
      }
    }
    return %0 : tensor<5x80x100x128xf32>
  }
  transform.named_sequence @__halide(%arg0: !transform.any_op) {
    %0 = transform.structured.match ops{["linalg.broadcast"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %1 = transform.structured.match ops{["linalg.generic"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    %2:2 = transform.split_handle %1 : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op, %forall_op = transform.structured.tile_using_forall %2#1 tile_sizes [0, 0, 0, 64] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %tiled_op_0, %forall_op_1 = transform.structured.tile_using_forall %tiled_op tile_sizes [1, 1, 5, 0] : (!transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op, %new_containing_op = transform.structured.fuse_into_containing_op %2#0 into %forall_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_2, %new_containing_op_3 = transform.structured.fuse_into_containing_op %fused_op into %forall_op_1 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_4, %new_containing_op_5 = transform.structured.fuse_into_containing_op %0 into %new_containing_op : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %fused_op_6, %new_containing_op_7 = transform.structured.fuse_into_containing_op %fused_op_4 into %new_containing_op_3 : (!transform.any_op, !transform.any_op) -> (!transform.any_op, !transform.any_op)
    %3 = transform.structured.match ops{["func.func"]} in %arg0 : (!transform.any_op) -> !transform.any_op
    transform.apply_patterns to %3 {
    } : !transform.any_op
    %fill_op, %split_linalg_op, %combining_linalg_op, %for_op = transform.structured.tile_reduction_using_for %fused_op_2 by tile_sizes = [0, 0, 0, 0, 1, 1, 1] : (!transform.any_op) -> (!transform.any_op, !transform.any_op, !transform.any_op, !transform.any_op)
    transform.yield 
  }
}


